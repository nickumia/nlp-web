<p>As humans, we have FIVE main senses, (1) Sight, (2) Hearing, (3) Touch, (4) Smell and (5) Taste.  There are possibly a few more senses that would make it on this list, but for simplicity's sake, we will only consider these five.  In our current age, computers are quite adept in seeing and listening to things in the world.  They also have a limited understanding of touch through force and other limit sensors.  However, for computers to gain a human-equivalent understanding of the world, they must have access to these missing senses.</p>

<h2 class="section-heading">Senses are very strange things.</h2>

<p>When you see a car, you see ..well.. a car.  As humans, we don't normally think about the light signals that interact with the photon receivers in our eyes that translate into chemical signals that are interpreted by our brain and somehow pixelated (or otherwise brought together) to form what we know of as a car.  This process occurs in nano, if not pico, seconds.  Similarly, we don't feel the vibrations in the air that resonate in our ears to form our brain's understanding of sound as a car horn.</p>

<p>Each of these processes consist of many, many different attributes that influence our understanding of a particular input to any of our sense facilities.  To a computer, we've summarized the "seeing process" in terms of colorized pixels and the "hearing process" in terms of frequency-variable waveforms.  Nevertheless, there's a plethora of information that gets lost in translation with both of these generalizations.  </p>

<!-- <a href="#"> -->
  <img class="img-fluid" src="https://injuryfirm.vegas/wp-content/uploads/2014/11/Las-Vegas-Car-vs-Pedestrian-Accident-Attorney.jpg" style="width:100%"alt="">
<!-- </a> -->
<span class="caption text-muted">Example Situation: Car hits biker.</span>

<p>We've made it the standard to categorize a picture as having, or not having, a particular object (e.g. a car, a bike, people ...).  After these categorizations are made, further processing needs to be done to make connections between the various elements.  </p>

<p><i>The car is facing a person with a bike. </i></p>

<p>Computers also need to account for things like blur and depth that change the meaning of situations.</p>  

<p><i>The person from the car is running towards the person with the bike.</i></p>

<p>Being able to form high-level ideas like this is one thing, but to understand the series of events that led to this event and also predict the series of events that may follow becomes  ..for a human like you or me, no problem!  ..for a machine, unwieldy. How many ideas must be accounted for to have as a complete understanding as a human would?</p>

<p>While I won't go into too much detail here, <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">Neural Networks</a> are a bubbling, re-invigorated technique that is used on the forefront of computer vision.  And others may be able to wow people with it's results.  But it fails to address basic questions such as, how much data does one need before they can say that they know "enough" to interpret a scenario?  Are our sense data-driven?  Can computers really say 1000 words about a picture?</p>

<h2 class="section-heading">What is a sense?  Is it universal?</h2>

<p>For all intents and purposes, a sense can be thought of as <b>a device that receives external stimuli, specifically a singular stimulus</b>.  With this definiton and the fact that we only have five senses, it seems as though we can clearly describe each of the senses and consolidate them into a single "master sense format".</p>

<p>There's a problem though.  (...Apart from three of the five senses not being fully accessible to computers haha...)  When each sense is taken in isolation, it is clear and well-defined.  But when they start to become combined, it gets a bit messy.  Take video, for example.  It is a combination of sight and hearing.  It's one thing to see something.  It's another to hear something.  And when you see what's making the sounds that you're hearing, it sort of makes things seem more natural.</p>

<p>It's normal to know that sounds don't occur without a source.  But knowing the exact source of a particular sound provides extra information that isn't really covered by the "sight" sense nor the "hearing" sense.  There's a link that's made between the two sense that contains its own information.</p>

<!-- <blockquote class="blockquote"></blockquote> -->

<h2 class="section-heading">Do we have control of our senses?</h2>

<p>This idea that senses are individual entities that work collectively to create our experience leads to the question of: How do we actually understand the senses that connect us to this world?  What does it really mean to have control of your own senses?</p>

<p>While I don't know if these questions can be answered universally, these questions should have specific answers relevant to each particular system that employs NLP practices.</p>

<p>Much of what makes makes these questions meaningful lies in the people and/or other things (living or not) that are being connected.  With regards to humans and machines, it's really on a case-by-case basis because each human culture would understand their senses in a slightly different way.  It's in such a way that some cultures would pick up different signals from their environment when compared to other cultures.</p>

<p>All machines are also not made for the same purposes.  So the information relevant to them may or may not be the same.  Having an understanding of the purpose of a machine helps to direct what information the system needs access to.  You wouldn't want to make a system to teach people to speak without having the system listen to and possibly look at the way people are speaking in order to help them completely.</p>

<h2 class="section-heading">Bridging gaps in sensory information</h2>

<p>Having the basic sense faculties to interact with people is important to have, but many times systems lack basic resources that enable them to operate.  While situations like these are not ideal, they are more than ubiquitous in the current world.  This leads to a lot of misinformed decisions and actions.</p>

<p>Working with incomplete information typically results in a lot of assumptions and otherwise self-informed details.  The idea is to keep these to a minimum by constantly asking about information that is missing.  It's better to be safe than sorry haha.</p>

<p>This means that systems need to have an excellent learning model, including learning about what situations need to be learned about.</p>

<p>Even though this level of the PLaN Framework is supposed to be "preprocessing" for higher-level understanding at the later levels, it's important to note that this level needs to be designed with the other levels in mind, NOT do the work of the other levels.</p>
